{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Ruler\n",
    "\n",
    "A **factory** in spaCy is a set of classes and functions preloaded in spaCy that perform set tasks. \n",
    "\n",
    "In the case of the **EntityRuler**, the factory at hand allows the user to create an EntityRuler, give it a set of instructions, and then use this instructions to find and label entities.\n",
    "\n",
    "\\\n",
    "**Once the user has created the EntityRuler and given it a set of instructions, the user can then add it to the spaCy pipeline as a new pipe.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipes and Pipelines\n",
    "\n",
    "A pipe is an individual component of a pipeline.\n",
    "\n",
    "There are different pipes that perform different tasks:\n",
    "- **The tokenizer** tokenizes the text into individual tokens.\n",
    "- **The Parser** parses the text\n",
    "- **The NER** identifies entities and labels them accordingly.\n",
    "\n",
    "\\\n",
    "**The sequence of a pipeline is crucial as later pipes can depend on earlier pipes.** This is important when creating custom spaCy pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating EntityRuler as a new pipe\n",
    "spaCy models do not come with a pre-built EntityRuler.\n",
    "\n",
    "The user must create an EntityRuler as a new pipe, give it instructions, and then add it to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules-based vs Machine Learning-based Approach\n",
    "\n",
    "**Rules-based appraoch**\n",
    "\n",
    "This is good for cases where the rules can be implemented and that the rules will always return true positives.\n",
    "\n",
    "An example of this can be the different ways that dates can be written.\n",
    "\n",
    "eg. \"Jan 1 2024\", \"January 1 2024\", \"01/01/24\", \"1st January 2024\", etc.\n",
    "\n",
    "\\\n",
    "**Machine Learning based approach**\n",
    "\n",
    "This is used when we cannot simply apply rules for what is or isn't a certain type of entity.\n",
    "\n",
    "An example would be recognizing names.\n",
    "\n",
    "We cannot possibly have a list of every possible first name, last name, prefix, and suffix in the world.\n",
    "\n",
    "This is where the machine learning-based approach would be used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"West Chestertenfieldville was referenced in Mr. Deeds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "West Chestertenfieldville GPE\n",
      "Deeds PERSON\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In an older version, West Chestertenfieldville was labeled as a Person not a GPE.**\n",
    "\n",
    "Mr. Deeds is a movie, not a person.\n",
    "\n",
    "We need to fix this using an EntityRuler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler = nlp.add_pipe(\"entity_ruler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False},\n",
       "  'entity_ruler': {'assigns': ['doc.ents', 'token.ent_type', 'token.ent_iob'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': [],\n",
       "  'entity_ruler': []},\n",
       " 'attrs': {'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []}}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see in the above output that the entity_ruler has now been added to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding patterns to the pipeline\n",
    "\n",
    "A list of dictionaries that defines what label the model will give when it finds something matching a certain pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    {\"label\": \"GPE\", \"pattern\": \"West Chestertenfieldville\"},\n",
    "    {\"label\": \"FILM\", \"pattern\": \"Mr. Deeds\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "West Chestertenfieldville GPE\n",
      "Deeds PERSON\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(text)\n",
    "for ent in doc2.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order of pipes matters!\n",
    "\n",
    "**The above code would not work since NER comes before Entity Ruler in the model pipeline**\n",
    "\n",
    "This means that the identification and labeling of entities takes place before the entity ruler.\n",
    "\n",
    "### What is the solution?\n",
    "There are 2 ways to solve this.\n",
    "\n",
    "1. We can give the EntityRuler the ability overwrite the NER.\n",
    "\n",
    "2. We can put the EntityRuler before the NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler = nlp2.add_pipe(\"entity_ruler\", before=\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'entity_ruler': {'assigns': ['doc.ents', 'token.ent_type', 'token.ent_iob'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'entity_ruler': [],\n",
       "  'ner': []},\n",
       " 'attrs': {'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['entity_ruler', 'ner'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['entity_ruler', 'ner'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['entity_ruler', 'ner'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []}}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp2.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "West Chestertenfieldville GPE\n",
      "Mr. Deeds FILM\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp2(text)\n",
    "for ent in doc3.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that the entity_ruler now comes before the NER in the above pipeline**.\n",
    "\n",
    "**We can see above that the entities are correctly recognized as GPE and as FILM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy can do a lot more\n",
    "We can not only look for exact matches for the pattern but we can also look for a list of linguistic features as well and label them with something we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
